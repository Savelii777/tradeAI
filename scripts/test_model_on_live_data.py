#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ –ª–∞–π–≤ –¥–∞–Ω–Ω—ã—Ö.
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
1. –ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è
2. –ö–∞–∫–∏–µ —Ñ–∏—á–∏ –ø–æ–ª—É—á–∞—é—Ç—Å—è
3. –ö–∞–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–µ–ª–∞–µ—Ç –º–æ–¥–µ–ª—å
4. –ü–æ—á–µ–º—É —Å–∏–≥–Ω–∞–ª—ã –Ω–µ –ø—Ä–æ—Ö–æ–¥—è—Ç —Ñ–∏–ª—å—Ç—Ä—ã
"""

import sys
from pathlib import Path
from datetime import datetime, timezone
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import joblib
import ccxt
from loguru import logger

sys.path.insert(0, str(Path(__file__).parent.parent))
from train_mtf import MTFFeatureEngine

# ============================================================
# CONFIG
# ============================================================
MODEL_DIR = Path("models/v8_improved")
LOOKBACK = 1500
MIN_CONF = 0.50
MIN_TIMING = 0.8
MIN_STRENGTH = 1.4

# ============================================================
# –§–£–ù–ö–¶–ò–ò (–¢–û–ß–ù–û –ö–ê–ö –í –õ–ê–ô–í–ï)
# ============================================================
def add_volume_features(df):
    """–¢–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–∞–π–≤–µ"""
    df = df.copy()
    df['vol_sma_20'] = df['volume'].rolling(20).mean()
    df['vol_ratio'] = df['volume'] / df['vol_sma_20']
    df['vol_zscore'] = (df['volume'] - df['vol_sma_20']) / df['volume'].rolling(20).std()
    
    # OBV —É–¥–∞–ª–µ–Ω (–∫–∞–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)
    
    df['vwap'] = (df['close'] * df['volume']).rolling(20).sum() / df['volume'].rolling(20).sum()
    df['price_vs_vwap'] = df['close'] / df['vwap'] - 1
    df['vol_momentum'] = df['volume'].pct_change(5)
    
    return df

def calculate_atr(df, period=14):
    high = df['high']
    low = df['low']
    close = df['close']
    tr = pd.concat([high - low, abs(high - close.shift()), abs(low - close.shift())], axis=1).max(axis=1)
    return tr.ewm(span=period, adjust=False).mean()

def prepare_features(data, mtf_fe):
    """–¢–æ—á–Ω–æ –∫–∞–∫ –≤ –ª–∞–π–≤–µ"""
    m1 = data['1m']
    m5 = data['5m']
    m15 = data['15m']
    
    if len(m1) < 50 or len(m5) < 50 or len(m15) < 50:
        return pd.DataFrame()
    
    # Ensure DatetimeIndex
    for df in [m1, m5, m15]:
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index, utc=True)
        df.sort_index(inplace=True)
    
    try:
        ft = mtf_fe.align_timeframes(m1, m5, m15)
        if len(ft) == 0:
            return pd.DataFrame()
        
        ft = ft.join(m5[['open', 'high', 'low', 'close', 'volume']])
        ft = add_volume_features(ft)
        ft['atr'] = calculate_atr(ft)
        
        # Fill NaN
        critical_cols = ['close', 'atr']
        ft = ft.dropna(subset=critical_cols)
        ft = ft.ffill().bfill()
        
        if ft.isna().any().any():
            ft = ft.fillna(0)
        
        return ft
    except Exception as e:
        logger.error(f"Error preparing features: {e}")
        return pd.DataFrame()

# ============================================================
# –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•
# ============================================================
def fetch_live_data(pair, binance):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ API (–∫–∞–∫ –≤ –ª–∞–π–≤–µ)"""
    data = {}
    for tf in ['1m', '5m', '15m']:
        try:
            candles = binance.fetch_ohlcv(pair, tf, limit=LOOKBACK)
            if not candles or len(candles) < 50:
                return None
            
            df = pd.DataFrame(candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
            df.set_index('timestamp', inplace=True)
            data[tf] = df
        except Exception as e:
            logger.error(f"Error fetching {pair} {tf}: {e}")
            return None
    
    return data

# ============================================================
# –ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ò
# ============================================================
def test_model_on_pair(pair, models, mtf_fe, binance):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π –ø–∞—Ä–µ"""
    print(f"\n{'='*70}")
    print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ {pair}")
    print(f"{'='*70}")
    
    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
    print(f"\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    data = fetch_live_data(pair, binance)
    if data is None:
        print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        return None
    
    print(f"   ‚úÖ 1m: {len(data['1m'])} —Å–≤–µ—á–µ–π")
    print(f"   ‚úÖ 5m: {len(data['5m'])} —Å–≤–µ—á–µ–π")
    print(f"   ‚úÖ 15m: {len(data['15m'])} —Å–≤–µ—á–µ–π")
    print(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–≤–µ—á–∞ 5m: {data['5m'].index[-1]}")
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ñ–∏—á–∏
    print(f"\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π...")
    df = prepare_features(data, mtf_fe)
    if df is None or len(df) < 2:
        print(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∏—á–∏")
        return None
    
    print(f"   ‚úÖ –§–∏—á–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö —Ñ–∏—á–µ–π –º–æ–¥–µ–ª–∏
    print(f"\n3Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏—á–µ–π –º–æ–¥–µ–ª–∏...")
    missing = [f for f in models['features'] if f not in df.columns]
    if missing:
        print(f"   ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∏—á–∏: {missing[:10]}")
        return None
    print(f"   ‚úÖ –í—Å–µ {len(models['features'])} —Ñ–∏—á–µ–π –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    # 4. –í–∑—è—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–∫—Ä—ã—Ç—É—é —Å–≤–µ—á—É (–∫–∞–∫ –≤ –ª–∞–π–≤–µ)
    print(f"\n4Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–≤–µ—á–∏...")
    row = df.iloc[[-2]]  # –ü—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω—è—è (–ø–æ—Å–ª–µ–¥–Ω—è—è –∑–∞–∫—Ä—ã—Ç–∞—è)
    last_candle_time = row.index[0]
    last_candle_close = row['close'].iloc[0]
    print(f"   –°–≤–µ—á–∞: {last_candle_time}")
    print(f"   Close: {last_candle_close:.6f}")
    
    # 5. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
    X = row[models['features']].values
    if pd.isna(X).any():
        print(f"   ‚ö†Ô∏è –ï—Å—Ç—å NaN –≤ —Ñ–∏—á–∞—Ö, –∑–∞–ø–æ–ª–Ω—è—é –Ω—É–ª—è–º–∏")
        X = np.nan_to_num(X)
    
    # 6. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print(f"\n5Ô∏è‚É£ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏...")
    dir_proba = models['direction'].predict_proba(X)
    dir_conf = float(np.max(dir_proba))
    dir_pred = int(np.argmax(dir_proba))
    timing_pred = float(models['timing'].predict(X)[0])
    strength_pred = float(models['strength'].predict(X)[0])
    
    direction_map = {0: 'SHORT', 1: 'SIDEWAYS', 2: 'LONG'}
    direction_str = direction_map[dir_pred]
    
    print(f"   Direction: {direction_str}")
    print(f"   Confidence: {dir_conf:.3f}")
    print(f"   Timing: {timing_pred:.3f} ATR")
    print(f"   Strength: {strength_pred:.2f}")
    
    # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤
    print(f"\n6Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤...")
    print(f"   –ü–æ—Ä–æ–≥–∏: Conf>={MIN_CONF}, Timing>={MIN_TIMING}, Strength>={MIN_STRENGTH}")
    
    rejected_reasons = []
    if dir_pred == 1:
        rejected_reasons.append(f"SIDEWAYS")
    if dir_conf < MIN_CONF:
        rejected_reasons.append(f"Conf({dir_conf:.3f}<{MIN_CONF})")
    if timing_pred < MIN_TIMING:
        rejected_reasons.append(f"Timing({timing_pred:.3f}<{MIN_TIMING})")
    if strength_pred < MIN_STRENGTH:
        rejected_reasons.append(f"Strength({strength_pred:.2f}<{MIN_STRENGTH})")
    
    if rejected_reasons:
        print(f"   ‚ùå –°–∏–≥–Ω–∞–ª –æ—Ç–∫–ª–æ–Ω–µ–Ω: {', '.join(rejected_reasons)}")
    else:
        print(f"   ‚úÖ –°–∏–≥–Ω–∞–ª –ø—Ä–æ—Ö–æ–¥–∏—Ç –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã!")
    
    # 8. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º N —Å–≤–µ—á–∞–º
    print(f"\n7Ô∏è‚É£ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º 20 —Å–≤–µ—á–∞–º...")
    last_20 = df.tail(20)
    X_all = last_20[models['features']].values
    X_all = np.nan_to_num(X_all)
    
    dir_proba_all = models['direction'].predict_proba(X_all)
    dir_preds_all = np.argmax(dir_proba_all, axis=1)
    
    long_count = np.sum(dir_preds_all == 2)
    short_count = np.sum(dir_preds_all == 0)
    sideways_count = np.sum(dir_preds_all == 1)
    
    print(f"   LONG: {long_count} ({long_count/20*100:.0f}%)")
    print(f"   SHORT: {short_count} ({short_count/20*100:.0f}%)")
    print(f"   SIDEWAYS: {sideways_count} ({sideways_count/20*100:.0f}%)")
    
    # 9. –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    timing_all = models['timing'].predict(X_all)
    strength_all = models['strength'].predict(X_all)
    conf_all = np.max(dir_proba_all, axis=1)
    
    print(f"\n8Ô∏è‚É£ –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20):")
    print(f"   Avg Confidence: {np.mean(conf_all):.3f}")
    print(f"   Avg Timing: {np.mean(timing_all):.3f} ATR")
    print(f"   Avg Strength: {np.mean(strength_all):.2f}")
    
    return {
        'pair': pair,
        'direction': direction_str,
        'conf': dir_conf,
        'timing': timing_pred,
        'strength': strength_pred,
        'passes': len(rejected_reasons) == 0,
        'rejected_reasons': rejected_reasons,
        'last_20_stats': {
            'long': long_count,
            'short': short_count,
            'sideways': sideways_count,
            'avg_conf': float(np.mean(conf_all)),
            'avg_timing': float(np.mean(timing_all)),
            'avg_strength': float(np.mean(strength_all))
        }
    }

# ============================================================
# MAIN
# ============================================================
def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--pair", type=str, default="BTC/USDT:USDT", help="Pair to test")
    parser.add_argument("--pairs", type=str, default=None, help="Comma-separated list of pairs")
    args = parser.parse_args()
    
    print("="*70)
    print("–ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ò –ù–ê –õ–ê–ô–í –î–ê–ù–ù–´–•")
    print("="*70)
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {MODEL_DIR}...")
    try:
        models = {
            'direction': joblib.load(MODEL_DIR / 'direction_model.joblib'),
            'timing': joblib.load(MODEL_DIR / 'timing_model.joblib'),
            'strength': joblib.load(MODEL_DIR / 'strength_model.joblib'),
            'features': joblib.load(MODEL_DIR / 'feature_names.joblib')
        }
        print(f"   ‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(models['features'])} —Ñ–∏—á–µ–π")
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
    mtf_fe = MTFFeatureEngine()
    binance = ccxt.binance({
        'timeout': 10000,
        'enableRateLimit': True,
        'options': {'defaultType': 'future'}
    })
    
    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–∞—Ä—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    if args.pairs:
        pairs = [p.strip() for p in args.pairs.split(',')]
    else:
        pairs = [args.pair]
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞–∂–¥—É—é –ø–∞—Ä—É
    results = []
    for pair in pairs:
        try:
            result = test_model_on_pair(pair, models, mtf_fe, binance)
            if result:
                results.append(result)
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {pair}: {e}")
            import traceback
            traceback.print_exc()
    
    # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    if results:
        print(f"\n{'='*70}")
        print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
        print(f"{'='*70}")
        
        passes = [r for r in results if r['passes']]
        print(f"\n‚úÖ –°–∏–≥–Ω–∞–ª–æ–≤ –ø—Ä–æ—Ö–æ–¥–∏—Ç —Ñ–∏–ª—å—Ç—Ä—ã: {len(passes)}/{len(results)}")
        
        if passes:
            print(f"\n–ü–∞—Ä–∞(—ã) —Å —Å–∏–≥–Ω–∞–ª–∞–º–∏:")
            for r in passes:
                print(f"  ‚úÖ {r['pair']}: {r['direction']} (Conf={r['conf']:.3f}, Timing={r['timing']:.2f}, Strength={r['strength']:.2f})")
        
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º:")
        all_sideways = sum(r['last_20_stats']['sideways'] for r in results)
        all_long = sum(r['last_20_stats']['long'] for r in results)
        all_short = sum(r['last_20_stats']['short'] for r in results)
        total = all_sideways + all_long + all_short
        
        if total > 0:
            print(f"   LONG: {all_long} ({all_long/total*100:.1f}%)")
            print(f"   SHORT: {all_short} ({all_short/total*100:.1f}%)")
            print(f"   SIDEWAYS: {all_sideways} ({all_sideways/total*100:.1f}%)")
        
        avg_conf = np.mean([r['last_20_stats']['avg_conf'] for r in results])
        avg_timing = np.mean([r['last_20_stats']['avg_timing'] for r in results])
        avg_strength = np.mean([r['last_20_stats']['avg_strength'] for r in results])
        
        print(f"\n   –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
        print(f"   Avg Confidence: {avg_conf:.3f}")
        print(f"   Avg Timing: {avg_timing:.3f} ATR")
        print(f"   Avg Strength: {avg_strength:.2f}")
        
        print(f"\nüí° –ê–Ω–∞–ª–∏–∑:")
        if all_sideways / total > 0.8:
            print(f"   ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç SIDEWAYS –≤ {all_sideways/total*100:.0f}% —Å–ª—É—á–∞–µ–≤")
            print(f"   ‚Üí –í–æ–∑–º–æ–∂–Ω–æ, –º–æ–¥–µ–ª—å –Ω–µ –∞–∫—Ç—É–∞–ª—å–Ω–∞ –∏–ª–∏ —Ä—ã–Ω–æ–∫ –≤ –±–æ–∫–æ–≤–æ–º —Ç—Ä–µ–Ω–¥–µ")
        if avg_strength < MIN_STRENGTH:
            print(f"   ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –°—Ä–µ–¥–Ω–∏–π Strength ({avg_strength:.2f}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ ({MIN_STRENGTH})")
            print(f"   ‚Üí –ú–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–∏—Ç —Å–∏–ª—å–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π")
        if avg_timing < MIN_TIMING:
            print(f"   ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞: –°—Ä–µ–¥–Ω–∏–π Timing ({avg_timing:.2f}) –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ ({MIN_TIMING})")
            print(f"   ‚Üí –ú–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–∏—Ç —Ö–æ—Ä–æ—à–∏—Ö —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞")

if __name__ == '__main__':
    main()

